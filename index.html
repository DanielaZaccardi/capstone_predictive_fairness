<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Summary of the Report</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <h1>Predictive Modeling and Fairness in Higher Education:<br>
      <span class="subtitle">A Case Study with FIU Admissions Data</span></h1>

    <div class="project-info">
      <p><strong>Course:</strong> IDC-6940: Capstone Course in Data Science</p>
      <p><strong>Authors:</strong> Osmel Cereijo and Daniela Zaccardi</p>
      <p><strong>Course Instructor:</strong> Dr. Ananda M. Mondal </p>
      <p><strong>Mentor:</strong> Dr. Agoritsa Polyzou </p>
    </div>
     
    <div class="resource-box">
      <h2>Resources</h2>
      <p>For detailed results, references, source code, and dataset, please see:</p>
      <ul>
        <li>
          Presentation Slides:
          <a href="https://github.com/DanielaZaccardi/capstone_predictive_fairness/blob/main/Capstone_Predictive_Fairness_Presentation.pdf" target="_blank">View Presentation Slides</a>
        </li>
        <li>
          GitHub Repository:
          <a href="https://github.com/DanielaZaccardi/capstone_predictive_fairness">View on GitHub</a>
        </li>
        <li>
          Dataset: FIU-provided dataset (restricted access)
    </li>
      </ul>
    </div>
            
    <h2>I. Introduction</h2>
      <h3>A. Background</h3>
      <p>Universities increasingly rely on predictive modeling and machine learning to support undergraduate admissions decisions, aiming to improve consistency and efficiency when evaluating large applicant pools. 
        While these models can achieve strong predictive performance, they also raise concerns about fairness, as historical admissions data may encode structural inequalities related to academic preparation, residency policies, or demographic characteristics. 
        If left unexamined, predictive systems risk reinforcing or amplifying these disparities. 
        This project examines both the predictive accuracy and fairness of machine learning models applied to real undergraduate admissions data from Florida International University (FIU).</p>
      
      <h3>B. Goals and Objectives</h3>
        <p>The goals of this project are to explore how machine learning models can develop bias accross different groups and investigate methods to alleviate said bias while  maintaining predictive accuracy.
          To achieve this, we focused on three key objectives:
        <ul>
          <li>Predict admissions outcomes using FIU’s Fall 2024 undergraduate applicant dataset and identify which features contribute most strongly to model predictions.</li>
          <li>Evaluate fairness across different demographic groups to determine whether models exhibit disparate outcomes or measurable bias.</li>
          <li>Investigate which bias mitigation methods are most effective at reducing disparities while retaining strong model performance.</li>
        </ul>
        </p>

      <h3>C. Research Questions</h3>
        <ul>
          <li>How accurately can we predict admissions decisions?</li>
          <li>What features most strongly influence predictions?</li>
          <li>Does the model have bias, and if yes, which groups are the most effected?​</li>
          <li>If bias exists, is it harmful or justifiable?​</li>
          <li>What methods can detect, measure, and mitigate bias?​</li>
          <li>How much bias can be effectively reduced without sacrificing accuracy?​</li>
        </ul>

      <h3>D. Motivation</h3>
        <p>Admissions decisions play a critical role in shaping students’ educational opportunities and long-term outcomes. 
          As predictive analytics become more integrated into institutional decision-making, there is growing legal, societal, and ethical pressure to ensure that these systems operate transparently and fairly. 
          Public scrutiny of admissions practices and increasing awareness of algorithmic bias highlight the need for rigorous evaluation of both model performance and equity. 
          Motivated by these concerns, this project seeks to provide evidence-based insight into how fairness issues emerge in admissions modeling and how they can be addressed responsibly without undermining model reliability.</p>
    
    <h2>II. Prior Art</h2>
      <p>Prior research on algorithmic decision-making in education has consistently emphasized the importance of explicitly measuring group-level fairness alongside predictive performance. Existing studies commonly evaluate bias using metrics such as statistical parity, disparate impact, equal opportunity, equalized odds, and subgroup error-rate differences, highlighting that overall accuracy alone is insufficient for assessing equitable model behavior. 
        A recurring theme in the literature is the need to examine baseline group disparities before building predictive models, as underlying data imbalances often drive observed fairness gaps.
        Across this body of work, interpretable models such as decision trees, random forests, and logistic regression are frequently used due to their transparency and suitability for educational data. While these models typically achieve high predictive accuracy, prior findings consistently show degraded performance for minority or underrepresented groups. Together, these results reinforce the necessity of systematic fairness evaluation and targeted mitigation strategies when applying machine learning to high-stakes contexts like admissions.</p>
    
    <h2>III. Challenges</h2>
        <ul>
          <li>Data quality and preprocessing</li>
          <li>Bias detection and measurement</li>
          <li>Bias mitigation trade-offs</li>
          <li>Model transparency and explainability</li>
        </ul>
    
    <h2>IV. Data Sources and Description</h2>
      
    <h3>A. Data Source and Size</h3>
      <p>This project uses an anonymized undergraduate admissions dataset provided by Florida International University (FIU) admissions for Fall 2024 applicants.</p>
      <p>After filtering to undergraduate applicants only, the final dataset contains ~30K applicants with 42 original features describing demographic attributes, academic background, standardized testing, and application details.</p>
        <img src="./figures/Sample_Applicant.png" alt="Sample Applicant Data" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">
    
    <h3>B. Data Quality & Challenges</h3>
      <img src="./figures/Data_Quality_Challenges.png" alt="Summary of Data Quality and Challenges" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

    <h3>C. Data Preparation & Constraints</h3>
      <p>The dataset contained substantial missing values, particularly for standardized test scores and GPA measures. 
        To address this, features with excessive missingness were removed, remaining values were consolidated where appropriate, and missingness indicators were added to preserve information without introducing bias.</p>
    
  <h2>Conclusion & Future Work</h2>
 
    <h3>Conclusion</h3>
      <p>
        In our research we determined that all the models performed consistently strong across the board.
        Our analysis of the fairness metrics revealed consistent disparities across certain minority groups.
        Our findings also indicate that alot of the issues presented throughout the research were mostly data-driven structural patterns, not model specific.
        Lastly we also investigated and found that the demographic features of our data were not the top predictors for the models.
      </p>
       
      <p>
        Our applied pre-processing techniques signficantly impoved the fairness metrics of our model.
        The predictive proformance of our models remained strong both when specifically targetting a group and also overall.
        We also determined that fairness can be improved without sacrificing to much accuracy from the models.
      </p>
 
    <h3>Future Work</h3>
      <p>
        We hope that in the future for this research we incoporate more semesters and additional demographic features that we saddly not provided in our original dataset.
        Things like the extra curriculars that the applciant has done, as well as other details that could help us infer more info of what the current situation for the applicant is.
        We would also like to explore more fairness-aware algoriths and bias mitigation techniques so that we could expand the potential of the models.
        As for our analysis we would like to explore more fairness metrics and also see how the models perform accross different intersectional groups.
        We would also want to split the analysis accross different application types to see if there are any trends that are specific to certain application type.
        Lastly we would want to explore deeper into trends happening overtime in relation to the semester cohorts being admitted.
      </p>
 
    <h2>Limitations</h2>
      <p>While this study provides meaningful insights into fairness and predictive modeling within undergraduate admissions, several limitations should be considered when interpreting the results.</p>
 
      <ul>
        <li>The analysis is based on FIU’s Fall 2024 applicant data, which reflects the unique demographic makeup and institutional policies of a large Hispanic-serving public university in Florida. As a result, patterns observed here may not generalize to other universities.</li>
        <li>Admissions outcomes in 2024 may have been influenced by geopolitical or policy-driven considerations outside the scope of this study.</li>
        <li>Certain features required assumptions due to missing or ambiguous information.</li>
        <li>FIU, like many public universities, operates under state-level policies that influence admissions decisions.</li>
        <li>Protected group definitions used in fairness evaluation (e.g., out-of-state vs. in-state, younger vs. older applicants) were tailored to this dataset. However, these definitions may differ in other states or institutions based on demographic composition, admissions priorities, or legal requirements.</li>
        <li>Each U.S. state operates under its own regulations regarding admissions, demographic reporting, and fairness considerations.</li>
      </ul>
    
  </div>
</body>
</html>
