<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Summary of the Report</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <h1>Predictive Modeling and Fairness in Higher Education:<br>
      <span class="subtitle">A Case Study with FIU Admissions Data</span></h1>

    <div class="project-info">
      <p><strong>Course:</strong> IDC-6940: Capstone Course in Data Science</p>
      <p><strong>Authors:</strong> Osmel Cereijo and Daniela Zaccardi</p>
      <p><strong>Course Instructor:</strong> Dr. Ananda M. Mondal </p>
      <p><strong>Mentor:</strong> Dr. Agoritsa Polyzou </p>
    </div>
     
    <div class="resource-box">
      <h2>Resources</h2>
      <p>For detailed results, references, source code, and dataset, please see:</p>
      <ul>
        <li>
          Presentation Slides:
          <a href="https://github.com/DanielaZaccardi/capstone_predictive_fairness/blob/main/Capstone_Predictive_Fairness_Presentation.pdf" target="_blank">View Presentation Slides</a>
        </li>
        <li>
          GitHub Repository:
          <a href="https://github.com/DanielaZaccardi/capstone_predictive_fairness">View on GitHub</a>
        </li>
        <li>
          Dataset: FIU-provided dataset (restricted access)
    </li>
      </ul>
    </div>
            
    <h2>I. Introduction</h2>
      <h3>A. Background</h3>
      <p>Universities increasingly rely on predictive modeling and machine learning to support undergraduate admissions decisions, aiming to improve consistency and efficiency when evaluating large applicant pools. 
        While these models can achieve strong predictive performance, they also raise concerns about fairness, as historical admissions data may encode structural inequalities related to academic preparation, residency policies, or demographic characteristics. 
        If left unexamined, predictive systems risk reinforcing or amplifying these disparities. 
        This project examines both the predictive accuracy and fairness of machine learning models applied to real undergraduate admissions data from Florida International University (FIU).</p>
      
      <h3>B. Goals and Objectives</h3>
        <p>The goals of this project are to explore how machine learning models can develop bias accross different groups and investigate methods to alleviate said bias while  maintaining predictive accuracy.
          To achieve this, we focused on three key objectives:
        <ul>
          <li>Predict admissions outcomes using FIU’s Fall 2024 undergraduate applicant dataset and identify which features contribute most strongly to model predictions.</li>
          <li>Evaluate fairness across different demographic groups to determine whether models exhibit disparate outcomes or measurable bias.</li>
          <li>Investigate which bias mitigation methods are most effective at reducing disparities while retaining strong model performance.</li>
        </ul>
        </p>

      <h3>C. Research Questions</h3>
        <ul>
          <li>How accurately can we predict admissions decisions?</li>
          <li>What features most strongly influence predictions?</li>
          <li>Does the model have bias, and if yes, which groups are the most effected?​</li>
          <li>If bias exists, is it harmful or justifiable?​</li>
          <li>What methods can detect, measure, and mitigate bias?​</li>
          <li>How much bias can be effectively reduced without sacrificing accuracy?​</li>
        </ul>

      <h3>D. Motivation</h3>
        <p>Admissions decisions play a critical role in shaping students’ educational opportunities and long-term outcomes. 
          As predictive analytics become more integrated into institutional decision-making, there is growing legal, societal, and ethical pressure to ensure that these systems operate transparently and fairly. 
          Public scrutiny of admissions practices and increasing awareness of algorithmic bias highlight the need for rigorous evaluation of both model performance and equity. 
          Motivated by these concerns, this project seeks to provide evidence-based insight into how fairness issues emerge in admissions modeling and how they can be addressed responsibly without undermining model reliability.</p>
    
    <h2>II. Prior Art</h2>
      <p>Prior research on algorithmic decision-making in education has consistently emphasized the importance of explicitly measuring group-level fairness alongside predictive performance. Existing studies commonly evaluate bias using metrics such as statistical parity, disparate impact, equal opportunity, equalized odds, and subgroup error-rate differences, highlighting that overall accuracy alone is insufficient for assessing equitable model behavior. 
        A recurring theme in the literature is the need to examine baseline group disparities before building predictive models, as underlying data imbalances often drive observed fairness gaps.
        Across this body of work, interpretable models such as decision trees, random forests, and logistic regression are frequently used due to their transparency and suitability for educational data. While these models typically achieve high predictive accuracy, prior findings consistently show degraded performance for minority or underrepresented groups. Together, these results reinforce the necessity of systematic fairness evaluation and targeted mitigation strategies when applying machine learning to high-stakes contexts like admissions.</p>
    
    <h2>III. Challenges</h2>
        <ul>
          <li>Data quality and preprocessing</li>
          <li>Bias detection and measurement</li>
          <li>Bias mitigation trade-offs</li>
          <li>Model transparency and explainability</li>
        </ul>
    
    <h2>IV. Data Sources and Description</h2>
      
    <h3>A. Data Source and Size</h3>
      <p>This project uses an anonymized undergraduate admissions dataset provided by Florida International University (FIU) admissions for Fall 2024 applicants.</p>
      <p>After filtering to undergraduate applicants only, the final dataset contains ~30K applicants with 42 original features describing demographic attributes, academic background, standardized testing, and application details.</p>
        <img src="./figures/Sample_Applicant.png" alt="Sample Applicant Data" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">
    
    <h3>B. Data Preparation & Constraints</h3>
      <p>The dataset contained substantial missing values, particularly for standardized test scores and GPA measures. 
        To address this, features with excessive missingness were removed, remaining values were consolidated where appropriate, and missingness indicators were added to preserve information without introducing bias.</p>
    <p>Bellow is a summary of the data preparation workflow:</p>
      <img src="./figures/Data_Preparation_Workflow.png" alt="Summary of Data Preparation Workflow" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

    <h3>C. List of Features</h3>
    <p>Following data cleaning, consolidation, and feature selection, the final set of features retained for modeling and fairness evaluation is shown below.</p>
    <img src="./figures/Reduced_Feature_List.png" alt="Summary of Data Preparation Workflow" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

    <h3>D. Data Overview</h3>
    <p>The final dataset represents a diverse undergraduate applicant population from FIU’s Fall 2024 admissions cycle. Approximately half of the applicants were admitted, reflecting a moderately selective admissions process. Most applicants were first-time-in-college students, traditional college age, and Florida residents, consistent with FIU’s role as a large public university serving the state population. The applicant pool is ethnically diverse, with Hispanic/Latino applicants representing the largest group, followed by non-resident alien, White, and Black or African American applicants. Overall, the dataset captures meaningful variation across academic background, application type, and demographic characteristics, providing a strong foundation for both predictive modeling and fairness analysis.</p>
    <img src="./figures/Data_Overview.png" alt="Data Overview Highlights" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

  <h2>V. EDA and Bias Investigation</h2>
    <h3>A. Visualization Findings</h3>
       <p>Exploratory visualizations revealed clear differences in admission outcomes across several demographic and application-related attributes. While gender showed relatively balanced admission rates, other factors such as Florida residency, application type, age, ethnicity, and region of birth exhibited substantial disparities. In-state applicants and non-first-time-in-college applicants experienced significantly higher admission rates, reflecting institutional policies and structural constraints. Additionally, older applicants and certain ethnic or geographic groups showed more favorable outcomes compared to younger and international applicants. These visual patterns suggest that disparities in admissions outcomes are largely driven by underlying data structure and institutional context rather than isolated model behavior, motivating deeper quantitative fairness analysis.</p>
    <img src="./figures/Summary_of_Differences.png" alt="Summary of Visualization Findings" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

    <h3>B. Quantitative Metrics Analysis</h3>
        <p>To quantify disparities observed in the exploratory visualizations, we evaluated group-level fairness using Selection Rate, Weighted Selection Rate, and Entropy difference metrics. These measures provide complementary perspectives on how admission outcomes vary across demographic groups.</p>
      <p>The below table summarizes the differences per metric and demographic feature. Entropy differences color-highlights represent: gray (neutral/no change), green (diversity increased), and red (diversity decreased).</p>
    <img src="./figures/Summary_of_Visualization_Findings.png" alt="Summary of Differences" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

<h2>VI. Methodology and Model Performance</h2>
     <h3>A. Experimental Set-Up</h3>
    <p>The experimental workflow for this study follows the below steps: </p>
    <ol>
      <li>Split the dataset into training (80%) and test (20%) sets using a stratified split to preserve the original admission rate.</li>
        <li>Applied data pre-processing on the training set, including discretization of continuous features using k-means clustering and one-hot encoding of categorical variables.</li>
          <li>Performed hyperparameter tuning using 5-fold cross-validation to identify optimal model configurations.</li>
            <li>Retrained each model using the selected hyperparameters on the full training set.</li>
              <li>Evaluated final model performance on the held-out test set and compared results across models.</li>
            </ol>
    <h3>B. Baseline Model Selection</h3>
    <p>Several models were evaluated for admissions prediction, all achieving strong performance. Random Forest emerged as the best-performing model and was therefore selected as the baseline for further fairness evaluation and mitigation.</p>
    <img src="./figures/model_table.png" alt="Model Performance Results" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">

    <p>Feature importance was analyzed across the top-performing models to identify which attributes most strongly influenced admissions predictions. Across all models, academic preparation and application-related features—such as admission type, high-school GPA, and standardized test scores, consistently ranked among the most influential predictors. In contrast, demographic attributes were not among the top drivers of model decisions, suggesting that predictive performance is largely guided by academic and structural factors rather than sensitive characteristics.</p>
    <img src="./figures/feature_of_importance.png" alt="Top 10 Feature Importance" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">
    <p>Note: Logistic Regression coefficients were normalized to sum to 1 for comparison purposes.</p>

  <h2>VII. Fairness Evaluation</h2>
    <p>To assess whether model predictions differ across demographic groups, we evaluated fairness using multiple group-level metrics. These include Statistical Parity and Disparate Impact, which capture differences in selection rates between privileged and unprivileged groups, as well as Equal Opportunity and Equalized Odds, which measure disparities in true positive and error rates across groups. Together, these metrics provide a comprehensive view of both outcome-level and error-based fairness.</p>

    <p>The table below summarizes fairness results for each demographic attribute, highlighting where disparities are minimal and where larger gaps persist. This comparison helps identify which groups are most affected and motivates the focus of subsequent bias-mitigation experiments.</p>

    <img src="./figures/fairness_performance_results.png" alt="Fairness Evaluation Results" style="width: 100%; max-width: 800px; display: block; margin: 20px auto;">
    
  <h2>Conclusion & Future Work</h2>
 
    <h3>Conclusion</h3>
      <p>
        In our research we determined that all the models performed consistently strong across the board.
        Our analysis of the fairness metrics revealed consistent disparities across certain minority groups.
        Our findings also indicate that alot of the issues presented throughout the research were mostly data-driven structural patterns, not model specific.
        Lastly we also investigated and found that the demographic features of our data were not the top predictors for the models.
      </p>
       
      <p>
        Our applied pre-processing techniques signficantly impoved the fairness metrics of our model.
        The predictive proformance of our models remained strong both when specifically targetting a group and also overall.
        We also determined that fairness can be improved without sacrificing to much accuracy from the models.
      </p>
 
    <h3>Future Work</h3>
      <p>
        We hope that in the future for this research we incoporate more semesters and additional demographic features that we saddly not provided in our original dataset.
        Things like the extra curriculars that the applciant has done, as well as other details that could help us infer more info of what the current situation for the applicant is.
        We would also like to explore more fairness-aware algoriths and bias mitigation techniques so that we could expand the potential of the models.
        As for our analysis we would like to explore more fairness metrics and also see how the models perform accross different intersectional groups.
        We would also want to split the analysis accross different application types to see if there are any trends that are specific to certain application type.
        Lastly we would want to explore deeper into trends happening overtime in relation to the semester cohorts being admitted.
      </p>
 
    <h2>Limitations</h2>
      <p>While this study provides meaningful insights into fairness and predictive modeling within undergraduate admissions, several limitations should be considered when interpreting the results.</p>
 
      <ul>
        <li>The analysis is based on FIU’s Fall 2024 applicant data, which reflects the unique demographic makeup and institutional policies of a large Hispanic-serving public university in Florida. As a result, patterns observed here may not generalize to other universities.</li>
        <li>Admissions outcomes in 2024 may have been influenced by geopolitical or policy-driven considerations outside the scope of this study.</li>
        <li>Certain features required assumptions due to missing or ambiguous information.</li>
        <li>FIU, like many public universities, operates under state-level policies that influence admissions decisions.</li>
        <li>Protected group definitions used in fairness evaluation (e.g., out-of-state vs. in-state, younger vs. older applicants) were tailored to this dataset. However, these definitions may differ in other states or institutions based on demographic composition, admissions priorities, or legal requirements.</li>
        <li>Each U.S. state operates under its own regulations regarding admissions, demographic reporting, and fairness considerations.</li>
      </ul>
    
  </div>
</body>
</html>
